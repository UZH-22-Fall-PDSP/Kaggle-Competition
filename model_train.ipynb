{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Dependancies"
      ],
      "metadata": {
        "id": "W-FPUzzXVq4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mount google drive"
      ],
      "metadata": {
        "id": "iBKOuItAVwXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xit9YePaVRp_",
        "outputId": "e24b26cf-e816-45b0-dc8d-01d6c9345a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/!_2022_fall/PDSP/kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc3uZakUVfwC",
        "outputId": "cfe815e4-8faa-4613-c9b6-0429c64b94a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/!_2022_fall/PDSP/kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import packages"
      ],
      "metadata": {
        "id": "r_U2M7Y6Vy-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "from gensim.models import FastText\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I26q0iexVlnY",
        "outputId": "b5a3f441-278a-47e9-e30c-56ac57ca6e1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset preparation"
      ],
      "metadata": {
        "id": "E9koUiklV8N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load dataset"
      ],
      "metadata": {
        "id": "W8Xy3s-jWoYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read recipe csv file\n",
        "df_train = pd.read_csv(\"./data/RAW_recipes.csv\")\n",
        "\n",
        "# extract relevant col\n",
        "steps = df_train['steps']\n",
        "reviews = df_train['description']\n",
        "ingredients = df_train['ingredients']"
      ],
      "metadata": {
        "id": "y7OMjdGyWDUS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### normalization & tokenizing "
      ],
      "metadata": {
        "id": "xDSmIIJIWg9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(document):\n",
        "  # remove special characters\n",
        "  document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "  # remove numbers \n",
        "  document = re.sub('[0-9]+', '', document)\n",
        "\n",
        "  # remove single characters\n",
        "  document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "  document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "  # substituting multiple spaces with single space\n",
        "  document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "  # converting to lowercase\n",
        "  dodument = document.lower()\n",
        "\n",
        "  # tokenizing \n",
        "  document = tokenizer.tokenize(document) \n",
        "\n",
        "  # remove stop words \n",
        "  document = [w for w in document if len(w) > 2 if not w in en_stop]\n",
        "  '''\n",
        "  # lemmatization \n",
        "  tokens = document.split()\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "  tokens = [word for word in tokens if word not in en_stop]\n",
        "  tokens = [word for word in tokens if len(word)>3]\n",
        "\n",
        "  res = ' '.join(tokens)\n",
        "  '''\n",
        "\n",
        "  return document"
      ],
      "metadata": {
        "id": "vk694nKAWZdU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = nltk.WordPunctTokenizer()\n",
        "\n",
        "# preprocessing dataset \n",
        "preprocessed_steps = [preprocessing(step) for step in steps]\n",
        "preprocessed_reviews = [preprocessing(review) for review in reviews]\n"
      ],
      "metadata": {
        "id": "BZ7ihrfiWq4J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# merge \n",
        "dataset = []\n",
        "dataset.extend(preprocessed_steps)\n",
        "dataset.extend(preprocessed_reviews)"
      ],
      "metadata": {
        "id": "aWpPt0Jmc999"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train FastText model\n",
        "\n",
        "* quick tutorial : https://github.com/PacktPublishing/fastText-Quick-Start-Guide/blob/master/chapter5/fasttext%20with%20gensim.ipynb\n",
        "\n",
        "* learn more about the gensim fastText model parameter : https://radimrehurek.com/gensim/models/fasttext.html\n",
        "\n",
        "  * window (int, optional) â€“ The maximum distance between the current and predicted word within a sentence."
      ],
      "metadata": {
        "id": "RiohIKh5W9ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fooddotcom_v1`  \n",
        " * dataset : steps + review\n",
        " * vector size : 100\n",
        " * model : skip-gram \n",
        "  \n",
        " `fooddotcom_v2`  \n",
        " * dataset : steps + review\n",
        " * vector size : 100\n",
        " * model : skip-gram \n",
        " * n_gram min max"
      ],
      "metadata": {
        "id": "5yScCW-KXin2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model2 = FastText(dataset, size=100, window=5, min_count=5, min_n=2, max_n=5, workers=4, sg=1)"
      ],
      "metadata": {
        "id": "rvb8a_HSXEbh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick test \n",
        "model.wv.most_similar(\"olive oil\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewcwuZ_6YVqe",
        "outputId": "dfaad76b-e7cf-4fa9-955c-7739876dcd2f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('olive', 0.8815826177597046),\n",
              " ('oliver', 0.8713400363922119),\n",
              " ('olivetomato', 0.8702647686004639),\n",
              " ('kalamata', 0.8153486251831055),\n",
              " ('olives', 0.8048213720321655),\n",
              " ('toscano', 0.7860913276672363),\n",
              " ('olivio', 0.7833059430122375),\n",
              " ('olio', 0.7641619443893433),\n",
              " ('pepperocini', 0.7613413333892822),\n",
              " ('calamata', 0.7546628713607788)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./model/fooddotcom_v2')"
      ],
      "metadata": {
        "id": "dsX-NoL6Y2B2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RB8Uc48YbHhF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}